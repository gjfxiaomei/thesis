% 本文件是示例论文的一部分
% 论文的主文件位于上级目录的 `bachelor.tex` 或 `master.tex`

\chapter{基于深度强化学习的单路口智能交通信号调度}
\section{引言}
随着城市道路上汽车数量的日益增长，交通拥堵情况变得越来越严重，极大地影响了人们的日常生活以及城市交通的运作。其实，交通拥堵通常是由于不同的车流为了争夺同一个“行驶资源”而造成的，而这些“行驶资源”就是不同道路汇集的交叉路口。
交通信号调度问题研究的就是如何高效以及安全的调度交叉路口的不同交通流量。早期，由于技术的不成熟以及成本的约束，城市交通管理通常使用基于预先定义规则的调度方法来控制交叉路口的信号灯，这种做法的好处是可以保证调度的安全性以及不需要更多的人为干预成本，
但是随着城市汽车数量的增加，这种方法已经难以高效地调度交通流量，因为它在调度时没有考虑路口的实时交通状况，只是按照预先定义的规则以固定的顺序改变信号灯的状态。

传统方法的失利促使了人们对智能交通信号调度方法的研究。与传统方法不同的是，智能交通信号调度方法能够根据实时的交通状况制定出最优的策略，从而最大程度地缓解交通拥堵。
在这些方法中，基于深度强化学习的智能交通信号调度方法在近些年逐渐成为了研究热点。首先神经网络的发展以及强化学习理论的成熟为这类方法的研究提供了技术支持。其次，强化学习的无模型特性使得它能够在训练中进行自我学习，而不需要外部的人为干涉，
与那些需要人为定期修改控制规则的调度方法相比，降低了模型部署之后的管理成本。

\section{相关工作}
虽然目前有很多基于强化学习的交通信号调度研究工作，但它们的差异主要体现在智能体的设计方面，包括以下三点：

\textbf{（1）状态表示}

状态表示是对当前环境的定量描述。一些常见的状态特征包括：
\begin{itemize}
  \item 队列长度（Queue length）：队列长度是车道上处于“等待”状态的车辆的数量。对于车辆“等待”状态，有不同的定义。在\inlinecite{wei2018intellilight}中，速度小于$0.1\text{米}/s$的车辆被认为处于等待状态；在\inlinecite{steingrover2005reinforcement,kuyer2008multiagent}中，等待车辆是指没有移动位置的车辆。
  \item 等待时间 (Waiting time)：车辆的等待时间定义为车辆处于“等待”状态的时间段。等待期的开始时间的定义可能有所不同，在\inlinecite{van2016coordinated,wei2018intellilight}，他们认为等待时间是从车辆移动的最后一个时间戳开始，而\inlinecite{brys2014distributed,pham2013learning}认为等待时间是从车辆进入路网开始的。
  \item 交通流量（Traffic volume）：交通流量定义为车道上的车辆数量，等于该车道上处于等待状态的车辆和行驶车辆的总和。
  \item 相位（Phase）：将相位信息作为状态特征，首先要将其进行量化，\inlinecite{van2016coordinated,wei2018intellilight}用当前相位在预先定义的信号相位组中的索引值来确定。
\end{itemize}
通常情况下，状态描述会整合多个特征，以获得对交通状况更全面的描述。

\textbf{（2）奖励设计}

在交通信号控制问题中，尽管最终的目标是使所有车辆的行驶时间最小化，但由于一些原因，行驶时间很难直接作为RL的有效奖励。首先，车辆的行驶时间不单单受交通信号的影响，还与其他的因素有关，例如车辆的速度。其次，当交通信号控制器事先不知道车辆的目的地时（现实世界中经常出现这种情况），优化网络中所有车辆的行驶时间变得尤为困难。在这种情况下，只有当网络中的多个交叉路口采取了多项行动时，才能在车辆完成行程后测量车辆的行驶时间。因此，奖励功能通常被定义为下列因素的加权和，这些因素在智能体采取动作后可以被即刻测量出。
\begin{itemize}
  \item 总队列长度：这里队列长度是所有incoming lanes的队列长度之和。\inlinecite{zheng2019diagnosing}证明了最小化队列长度相当于最小化所有车辆的行驶时间。
  \item 吞吐量：吞吐量定义为在最后一个动作后的特定时间间隔内通过交叉路口或离开网络的车辆总数\cite{aslani2017adaptive,wei2018intellilight}。
  \item 速度：一个典型的奖励是取道路网中所有车辆的平均速度，平均速度越高意味着车辆行驶到目的地的速度越快，时间也就越短\cite{casas2017deep,van2016coordinated}。
  \item 信号变化频率：信号改变的频率被定义为在某一时间段内信号改变的次数。直观地说，学到的政策不应该导致闪烁，即频繁地改变交通信号，因为车辆通过交叉口的有效绿色时间可能会减少\cite{van2016coordinated,wei2018intellilight}。
  \item 停车次数：使用网络中所有车辆的平均停车次数作为奖励。直观地说，停车次数越少，交通就越顺畅\cite{van2016coordinated}。
  \item 路口压力：Wei在\inlinecite{wei2019presslight}工作中，将路口压力定义为每个交通运动的绝对压力之和。直观地说，较高的压力表明进站车道和出站车道数量之间的不平衡程度较高。
\end{itemize}

\textbf{（3）学习算法}

根据估计潜在奖励和选择动作的不同，现有基于强化学习的智能交通调度方法可以分为两类，分别是Value-based以及Policy-based，具体如\autoref{tab:rl-tsc-classfication}所示。
\begin{table}[htb]
    \caption{基于深度强化学习的智能交通信号调度方法分类\label{tab:rl-tsc-classfication}}
    \begin{tabular}{lll}
      \toprule
      方法 & 相关工作 & 优点\\
      \midrule
      Value-based & \inlinecite{abdoos2011traffic,abdoos2014hierarchical,arel2010reinforcement,van2016coordinated,wei2019colight,wei2019presslight,wei2018intellilight,zheng2019diagnosing,zang2020metalight} & \tabincell{l}{使用策略评估和策略控制来逼近最优策略，\\更易于理解。}\\
      \hline
      Policy-based& \inlinecite{aslani2017adaptive,aslani2019developing,aslani2018traffic,chu2019multi,rizzo2019time,zheng2019learning} & 直接学习控制策略，收敛速度更快。\\
      \bottomrule
    \end{tabular}
\end{table}

Value-based的方法通过近似价值值函数或动作价值函数的方式来隐式地学习控制策略。通常，为了避免模型陷入局部最优，这类方法在选择动作时会使用$\epsilon-greedy$的选择机制，从而提高探索效率，随着$\epsilon$的减小，模型也将趋于稳定。
此外，这类方法只能处理离散工作。

Policy-based的方法直接对控制策略进行建模并在迭代过程中对其进行更新。这类方法的优势是它不要求动作必须是离散的。此外，它自身就可以学习随机策略并不断探索可能更有价值的动作，因此不需要使用$\epsilon-greedy$的动作选择机制。

\section{研究目标}
虽然目前已经有不少基于强化学习的智能交通信号控制的研究，但是已有的方法更多的只注重提高通行效率，例如最小化队列长度或者最大化吞吐量，而忽略了公平性问题。
事实上，这样的目标会导致学习到一个有偏见的策略，即为了最大化系统的通行效率，控制模型会倾向于优先调度交通流量大的车道（即将这些交通流量对应的信号变为绿色），而忽略那些交通流量小的车道，因为这样做对整个路口的通行效率增益更高，但是这中做法会导致那些在交通流量小的车道上的车辆出现“饥饿等待”的情况，甚至在一些极端情况下会导致这些车辆要等待很长的时间才能够得到通过路口的机会。 更极端一点会出现如\autoref{fig:last-vehicle}所示的"last-vehicle"情况：N-S方向的道路上有源源不断的车辆到达，而E-W方向上是该车流的最后一辆车。
显然，为了最大化通行效率，会优先放行N-S方向上的车流，而E-W车道上的车要等所有的所有的车流通过才能够得到响应，这对E-W方向上的车来说是极其不公平的。

\begin{figure}[t]
  \includegraphics[width=0.9\textwidth]{fig/lastvehicle.pdf}
  \caption{"last-vehicle"情况示意图}
  \label{fig:last-vehicle}
\end{figure}
本工作的研究目标是在提高系统通行效率的同时（最小化平均通行时间），保证调度的相对公平性，即每条车道能够有尽可能相同的服务延迟（得到放行所需的时间）。

\section{系统设计}
本工作同样使用基于深度强化学习的智能交通信号调度框架，针对我们的研究目标，我们在已有方法的智能体设计上进行了改进，并给出了我们的系统框架。
\subsection{智能体设计}
\textbf{（1）状态表示}

在$t$时刻的状态$S(t)$由以下几个部分组成：
\begin{enumerate}
\item 交通流量：$\boldsymbol{V}(t)=\{V_1(t),V_2(t),\cdots,V_M(t)\}$。其中 $V_i(t)$表示第$i$条进近车道上车的数量。值得注意的是，由于右转不受限于信号灯的特殊性，这里我们不考虑右车道的交通流量。
\item 平均吞吐量：$\boldsymbol{\overline{L}}(t)=\{\overline{L}_1(t),\overline{L}_2(t),\cdots,\overline{L}_M(t)\}$。其中$\overline{L}_i(t)$表示第$i$条进近车道的平均吞吐量。同上，不考虑右车道的平均吞吐量。
\item 信号相位：$\boldsymbol{P}(t)$是当前信号相位的数字化表示，$1$表示绿色，可以通行；$0$表示红色，禁止通行。
\end{enumerate}
所以$S(t)=\{\boldsymbol{V}(t) || \boldsymbol{\overline{L}}(t) || \boldsymbol{P}(t) \}$

\textbf{（2）动作选择机制}

在本文中，动作选择机制是每次选择即将转换的信号相位。之后，交通信号灯将转换到这一新的相位并持续$\Delta t$的时间。为了安全起见，我们在两个不同的信号相位之间插入了3秒的黄色信号和2秒的红色信号。如果新选择的相位和当前相位相同，则不插入黄色和红色信号，以确保交通流畅。

\textbf{（3）奖励函数设计}

为了能够在效率和公平性之间得到一个良好的平衡，我们借鉴了PFS（Proportional Fair Schedule）的调度思想。PFS是一种应用于无线通信领域的调度算法，它能够在最大化吞吐量的同时保证每个用户的比例公平，其中比例的定义如下：
\begin{definition}[比例公平]
\label{def:PFS}
一个调度方案$P$是比例公平的当且仅当任何其他调度方案$S$满足下列条件：
\begin{align}
    \sum_{i \in U} \frac{R_{i}^{(S)}-R_{i}^{(P)}}{R_{i}^{(P)}} \leq 0
\end{align}
其中$U$是服务的用户集合，$R_i^{(S)}$表示在调度方案$S$下，用户$i$的平均速率。
\end{definition}
上述的约束表明，一个用户在分配上的任何正向变化必须导致系统的平均负向变化。在单载波无线网络情况下[20]，每次只允许一个用户传输，比例公平性的实现是通过每次将唯一的载波（或信道）按照以下规则分配给用户来实现的。
\begin{align}
    i^*=\underset{i\in U}{\arg \max } \frac{I_{i}}{\bar{T}_{i}}
\end{align}
其中$I_i$是根据实时信道条件估计的用户$i$瞬时吞吐量，$T_i$是用户$i$的平均吞吐量。这一原则意味着具有更高瞬时吞吐量的用户应具有更高的优先级来访问信道资源，以提高系统的整体吞吐量，同时，其平均吞吐量越小越有可能被调度，以保证一定程度的公平性。
我们将这一思想拓展到交通信号控制领域，然后设计了一个可以在效率和公平之间提供良好的平衡的奖励函数，如下所示：
\begin{align}
\label{eq:reward}
    r = -\sum_{i=1}^{M} \frac{Q_i(t)}{\overline{L}_i(t) + \delta},
\end{align}
其中$Q_i(t)$ 和 $\overline{L}_i(t)$ 分别是第$i$条进近车道的队列长度和平均吞吐量。在每一次调度后（这里，我们将一次动作选择视作一次调度），$\overline{L}_i(t)$ 按照以下方式进行更新：
\begin{align}
    \label{eq:li-update}
    \overline{L}_i(t) = (1-\frac{1}{W})\overline{L}_i(t-1) + \frac{1}{W}L_i(t),
\end{align}
其中$L_i(t)$是此次调度中车道$i$上得到放行的车的数量，$W$是一个平衡通行效率和公平性的参数。另外，为了避免公式\ref{eq:reward}的分母为0， 我们加上了一个可以忽略不计的正数$\delta$。

\begin{figure}[htb]
  \includegraphics[width=1.0\textwidth]{fig/sysmodel.pdf}
  \caption{单路口场景下信号调度智能体与环境交互示意图}
  \label{fig:system-model}
\end{figure}

\subsection{系统框架}
如\autoref{fig:system-model}所示，这里我们用DQN作为学习算法，并且采用经验回放\cite{mnih2015human}（experience replay）方法定期提取样本来更新模型，具体算法过程如\autoref{alg:dqn-single}所示。
\begin{breakablealgorithm}
    \caption{基于深度强化学习的智能交通信号控制模型训练流程}
    \label{alg:dqn-single}
    \begin{algorithmic}[1] %每行显示行号  
        \Require 
        $E$ : 学习片段数 \newline
        $T$ : 每个学习片段的步数 \newline
        $b$ : 学习经验数 \newline
        $\epsilon$ : 随机选择动作概率 \newline
        $\gamma$ : 折扣因子 \newline
        $\Delta t$ : 信号维持时间
        \For{$episode=1,E$}  
            \State 初始化环境。
            \State 生成车辆。
            \For{$t=1,T$}
                \State 从环境中获取状态观测$s_t$。
                \State 生成一个0到1之间的随机数$rand$。
                \If{ $rand < \epsilon$}
                    \State 从动作空间中随机采样一个动作$a_t$。
                \Else
                    \State 使用DQN模型选择动作：$a_t = \mathop{\arg\max}_a \mathcal{Q}(s_t,a;\theta)$。
                \EndIf
                \State 将当前信号更改为$a_t$并维持$\delta t$秒时间。
                \State 更新每条车道的平均吞吐量。
                \State 环境转移到新的状态$s_{t+1}$并返回一个奖励$r_{t+1}$。
                \State 将经验$(s_t,a_t,r_{t+1},s_{t+1})$ 存储到经验回放池M中。
                \If{$|M|>b$}{
                    \State 从经验回放池M中随机采样$b$条经验数据。
                    \State 计算损失函数 $\mathcal{L}_j$:
                    $\mathcal{L}_Q=[ r_{t+1}+\gamma \mathop{\arg\max}_{a^{'}} \mathcal{Q}(s_{t+1},a^{'};\theta)-\mathcal{Q}(s_t,a_t;\theta)]^2$。
                    \State 更新DQN参数。
                }
                \EndIf
            \EndFor
        \EndFor  
    \end{algorithmic}  
\end{breakablealgorithm}  

\section{实验}
\subsection{环境介绍}
实验在SUMO（simulation of Urban MObility）\footnote{http://sumo.dlr.de/index.html}仿真平台上进行，利用该模拟器可以方便地实时获取车辆状态，并通过改变交通信号来控制交通运行。我们实现了一个四路交叉口作为我们的实验场景，
交叉口与四个150米长的路段相连，每条道路有三条引入车道和三条引出车道。

我们将N-S方向的道路设置为主干道，车辆到达量更多，将W-E方向的道路设置为次干道，车辆到达量较少。车辆到达服从泊松分布，这里我们设置N-S方向道路的交通流量比率为$\rho$，W-E方向道路的交通流量比率为$1-\rho$，$\rho$值越高，交通流量不平衡的状况越严重。
为了对我们的方法进行综合评价，我们在不同的$\rho$值下进行了实验。注意，为了简化环境，这里我们不考虑行人交通的影响。

\subsection{评价指标}
我们使用以下指标来评估不同方法的效率和公平性表现：
\begin{itemize}
    \item 行驶时间：车辆行驶时间是指车辆进出路口的时间差。现有的大部分工作都集中在最小化所有车辆通过交叉路口的平均行驶时间。
    \item 延误时间：车辆延误时间是车辆通过交叉路口的实际时间与预期时间（以最高限速通过交叉路口所需的时间）之间的差值。
    \item 驾驶体验得分：此外，我们提出了一种新的评价指标，称为驾驶体验得分（Driving Experience Score，DES），来量化驾驶员的满意度，具体评分标准见下表：
        \begin{table}[htb]
            \caption{驾驶体验得分标准}
            \begin{tabular}{cc}
            \toprule
            延误时间（s） & DES \\
            \midrule
            $d \leq 40$ & 5\\
            $40 < d \leq 80$ & 4 \\
            $80 < d \leq 120$ & 3 \\
            $120 < d \leq 160$ & 2 \\
            $d > 160$ & 1 \\ 
            \bottomrule
            \end{tabular}
        \end{table}
        事实上，可能有更多的因素需要考虑（如燃油消耗），但是这里的目的是为了缓解车辆的过度延误情况，因此我们这里用延误时间作为评价标准。
\end{itemize}
\subsection{比较方法}
\begin{itemize}
    \item FT(Fixed-Time Control\cite{miller1963settings})：这种方法以预先设定的方式循环改变信号。
    \item SOTL(Self-Organizing Traffic Light Control\cite{cools2013self})：这是一种根据预先设定的阈值来改变信号的自适应方法。如果等待的车辆数量超过了这个阈值，则切换到下一个信号相位。
    \item LIT\cite{zheng2019diagnosing}：这是一种基于学习的方法，比大多数现有的致力于提高通行效率的方法效果更好。
    \item FIT(\textbf{F}airness-aware \textbf{I}ntelligent \textbf{T}raffic Light Control)：我们的方法。
\end{itemize}
\subsection{性能评估}
首先，我们通过实验评估了不同方法的通行效率的表现，为了得到一个综合的结果，我们在不同的$\rho$值下进行了实验，实验结果如\autoref{fig:efficiency}所示。
可以观察到，我们的方法（FIT）的车辆通过路口的平均行驶时间远低于传统方法（行驶时间越短意味着效率越高），并且仅略低于只注重效率的LIT方法。
\begin{figure}[htb]
    \includegraphics[width=0.9\textwidth]{fig/efficiency.pdf}
    \caption{不同交通负载情况下四种方法的路口通行效率表现}
    \label{fig:efficiency}
\end{figure}

其次，由于我们的主要研究目标是公平性，我们首先分析了在使用不同方法下每条车道的延误情况。这里我们用JFI（Jain Fairness Index）来量化公平性指标，JFI的计算方式如下：
\begin{align}
    \mathcal{J}=\frac{\left(\sum_{i=1}^{M} \bar{D}_{i}\right)^{2}}{M \sum_{i=1}^{M} \bar{D}_{i}^{2}},
\end{align}
其中$\bar{D}_{i}$是车道$i$的平均延误时间。当每个车道具有相同的平均延误时间时，JFI的值达到最大值，即1。\autoref{fig:fairness}展示了四种方法在不同$\rho$值下的平均延迟的JFI表现。从中我们可以看出FT和LIT的JFI值随着交通不平衡情况的加剧（即$\rho$值越大）而减小，而FIT任然能偶保持较高的值，并且高于同样能够保持稳定JFI值的SOTL方法。
\begin{figure}[htb]
    \includegraphics[width=0.9\textwidth]{fig/fairness.pdf}
    \caption{不同交通负载情况下四种方法的调度公平性表现}
    \label{fig:fairness}
\end{figure}

然后，我们更加详细地研究了不同方法的延误情况。下面我们具体分析在主干道和支干道上四种方法在不同的$\rho$值情况下车辆延误时间分布情况。
\begin{figure}[htb]
    \includegraphics[width=0.9\textwidth]{fig/major_delay.pdf}
    \caption{不同交通负载情况下四种方法在主干道(N-S方向)的车辆延误时间分布}
    \label{fig:major_delay}
\end{figure}
从\autoref{fig:major_delay}中我们可以看出，在主干道上，基于学习的方法（LIT和FIT）比传统方法（FT和SOTL）具有更低的延误时间，虽然SOTL方法整体上延迟也比较低，但是会有很多极端值，最高的延误时间甚至超过800秒。

从\autoref{fig:minor_delay}中我们可以看出，在支干道上，随着$\rho$值的增加（即交通不平衡情况的加重），原先在主干道上表现优异的LIT方法性能开始恶化（），但是FIT依然能够保持一个相对低的延迟。

\begin{figure}[htb]
    \includegraphics[width=0.9\textwidth]{fig/minor_delay.pdf}
    \caption{不同交通负载情况下四种方法在支干道(N-S方向)的车辆延误时间分布（单位：秒）}
    \label{fig:minor_delay}
\end{figure}

我们研究了不同方法的驾驶体验得分情况，\autoref{fig:des}展示了在$\rho=0.75$的情况下不同方法的驾驶体验得分分布情况。从中我们可以看出，FT方法超过半数的驾驶体验的分都是1分，由此可以看出该方法的不灵活。对于SOTL方法而言，虽然他的5分的比例最高，但是其得分分布的方差也是最高的。FIT的得分分布与LIT相似，但FIT的方差低于LIT，在以牺牲少量效率为代价的前提下。
\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{fig/des.pdf}
    \caption{在$\rho=0.75$的交通负载情况下四种方法的驾驶体验得分分布}
    \label{fig:des}
\end{figure}

最后，我们研究了\autoref{eq:li-update}中$W$参数的取值对模型性能的影响，因为$W$是用来平衡效率和公平性的，不同的值会导致学习到不同的策略，从\autoref{fig:window-size}中我们可以看出$W$值越高，越有利于调度的公平性。相反，$W$值越小，路口的通行效率就越高。具体来说，当$W=1$时，我们方法FIT的整体性能（包括通行效率和公平性）接近于LIT。
\begin{figure}[htb]
    \subfloat[\autoref{eq:li-update}中的参数$W$的取值对通行效率的影响\label{fig:window-size-efficiency}]{\includegraphics[width=0.45\textwidth]{fig/window_efficiency.pdf}}
    \subfloat[\autoref{eq:li-update}中的参数$W$的取值对调度公平性的影响\label{fig:window-size-fairness}]{\includegraphics[width=0.45\textwidth]{fig/window_fairness.pdf}}
    \caption{\autoref{eq:li-update}中的参数$W$的取值对通行效率和调度公平性的影响}
    \label{fig:window-size}
  \end{figure}

\section{本章小结}
本章首先对比了传统交通信号调度方法和基于深度强化学习的智能交通信号调度方法的优缺点，并总结了近些年来基于深度强化学习的智能交通调度研究工作，然后分析了这些工作的不足并进一步提出了本章工作的研究目标。通过引入无线通信网络中的PFS调度方法并配合深度强化学习，
我给出了一个具有公平感知能力的模型，这个模型在提高通信效率的同时能够确保调度的相对公平性。最后，我们在仿真环境中进行了大量的实验来验证我们模型的效果，并与已有方法进行对比，进一步阐述了我们模型在公平性方面的性能提升。