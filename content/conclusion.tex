\chapter{总结与展望}
\section{工作总结}
本文主要研究了基于深度强化学习的智能交通信号调度问题，包括单路口场景以及多路口场景下的信号调度问题。首先我们总结了一些传统的交通信号控制方法，这些方法要么受制于一些严苛的假设条件导致难以应用于实际交通控制中，要么缺乏对实时交通状况的考虑导致无法高效地缓解交通拥堵的情况。
然后我们总结了一些已有的使用深度强化学习解决智能交通信号调度问题的工作，在不同的调度场景下（单路口场景和多路口场景），我们对这些已有工作的进行了系统的分析，并指出他们存在的不足，最后我们针对这些不足提出新的解决方案。

对于单路口场景下的交通信号控制，已有的基于学习的方法更多的注重于提高通行效率，而忽略了公平性问题。在本文中，我们提出了一个新的模型可以在保证通行效率的同时，兼顾到对公平性的考虑。最后，我们在仿真环境中进行了大量的实验来验证我们模型的效果，并与已有方法进行对比，进一步阐述了我们模型在公平性方面的性能提升。

对于多路口场景下的交通信号控制，我们使用了IRL with communication的协调控制框架，并提出了一种新的将道路网建模成图的建模方式，在此基础上，我们设计了一种新的信息交互模块，通过这个信息交互模块，智能体在提取邻近节点的信息时可以剔除那些对自己无用的信息。最后我们在仿真环境中分别就合成交通数据和真实交通数据进行了实验，并与已有方法进行对比，进一步阐述了我们的模型在通行效率和学习速度上的提升。

\section{未来展望}
虽然目前有很多使用强化学习来解决智能交通信号调度的工作，并且也取得了不错的效果，但是任然有一些问题值得被深入研究：

\textbf{（1）信用分配问题}

信用分配问题是强化学习领域中被广泛研究的问题之一，它考虑的是对一个动作的成功（或失败的惩罚）的信用分配，即一个动作对于最终任务的实现有多大的贡献度，或者对于最终任务的失败应该承担多大的责任。在交通信号控制问题中，交通状况是交通信号控制器所采取的若干行动的结果，这带来了两个问题。(1)一个行动可能在几步行动后仍有效果；(2)每个时间戳的奖励是之前几个动作的组合结果。
在常规的强化学习应用环境中（例如，Atari游戏或围棋），有时会把一个训练集的最终得分分配给与这一集相关的所有动作，而交通信号控制问题中的动作从执行到产生影响可能有一个时间间隔，这个时间间隔可能是动态变化的，需要进一步研究。

\textbf{（2）Bus-Priority问题}

Bus-Priority是指在交通信号调度中，公共交通车辆相较于普通车辆应该具有更高的调度优先级，以鼓励人们在日常出行中乘坐公共交通用具。但是目前很多工作为了简化控制难度，都是无区别的对待道路上的交通流量。

\textbf{（3）实地测试问题}

目前有很多使用强化学习来解决智能交通信号调度的工作都只停留在仿真阶段，即实验场景的搭建以及效果的验证都是通过仿真器完成的，要想将这些模型部署到现实生活中的信号灯上还需要更多的研究和实地测试。但是随着车联网技术的发展使得我们可以实时地获取道路上车辆的信息，加上最近人工智能技术的崛起，
一些基于学习的交通信号调度算法可以在与环境的交互中不断的提升自身的性能，这些因素为进一步实现真实道路场景下的智能交通信号控制提供了技术支持。

\textbf{（4）安全性问题}

如何使强化学习智能体在物理环境中可以被接受是未来研究的一个重要方向。虽然强化学习方法从试错中学习，但在现实世界中，学习成本可能是关键的，甚至是致命的，因为交通信号的故障可能导致很严重的事故。因此，如何在强化学习中采用风险管理从而防止在智能体的学习过程中和学习结束后出现不必要的行为是十分关键的问题。

